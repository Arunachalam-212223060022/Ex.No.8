Exp 8: Reconstructing an Image Using Prompt-Based Image Generation
Date:
Reg. No.:
Aim

To explore how text-to-image generation models can recreate an existing image by crafting detailed prompts.
The objective is to identify key elements of the image—such as lighting, style, color, and composition—and gradually refine the prompt until the generated output closely resembles the original.

Procedure
1. Analyze the Image

Carefully observe the reference image and identify:

Subjects/Objects (people, animals, items)

Colors & Tones (warm, cool, contrast, highlights)

Textures (smooth, glossy, rough, metallic, matte)

Lighting (dim, bright, glowing, harsh shadows)

Background (indoor/outdoor, cluttered/simple)

Composition (framing, perspective, focal point)

Style (photo-realistic, artistic, cinematic, minimalistic)

2. Create the Basic Prompt

Write a simple description capturing the core elements.
Example:

“A woman sitting beside a window reading a book.”

3. Add More Descriptive Details

Enhance the prompt with:

Colors

Mood

Specific objects

Lighting direction

Atmosphere

Example:

“A woman reading beside a large window during a rainy evening, with soft blue light reflecting on her face.”

4. Identify Style or Art Influence

If the image has a specific style, include it:

Watercolor

Cinematic

Digital art

Photorealistic

Moody lighting

Example:

“A cinematic illustration with soft ambient lighting.”

5. Fine-Tune the Prompt Further

Include more details about textures, reflections, weather conditions, or background elements.

Example:

“Raindrops on the window create blurred city lights, indoor plants fill the room, and a warm lantern glows next to her.”

6. Generate the Image

Use a text-to-image model such as:

DALL·E

Stable Diffusion

MidJourney

Paste your refined prompt into the tool to generate the image.

7. Compare with the Original

Evaluate:

Accuracy of colors

Similarity of composition

Lighting correctness

Texture quality

Subject placement

Overall mood

Refine and repeat if needed.

Tools / LLMs Used for Image Generation
DALL·E (OpenAI)

A powerful tool capable of producing highly detailed images from textual prompts.

Stable Diffusion

An open-source model that allows customization and fine-tuning of outputs.

MidJourney

Known for artistic, cinematic, and visually rich generations.

Instructions

Observe the image and note all important visual elements.

Write a basic prompt summarizing the main subject.

Add details such as color, lighting, style, textures, etc.

Choose an AI model and input the refined prompt.

Iterate by comparing outputs and adjusting wording.

Document the final prompt and image results.

Deliverables

Original Image — Provided reference.

Final Generated Image — Output generated by the refined prompt.

Prompts Used — All versions of prompts crafted during the experiment.

Comparison Report — Similarities, differences, and improvements made.

Conclusion

Text-to-image models can recreate images with impressive accuracy when given precise, descriptive prompts.
The success of reproduction depends heavily on prompt quality, refinement, and understanding of visual elements.
This experiment shows the value of iterative prompt engineering when working with AI-generated imagery.
